{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Entropy Constraint Inference from IRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Entropy IRL (Zeibert et al. 2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The world is described as a Morkov Decision Process(MDP)\n",
    "    - A tuple of $(S, A, T, R, \\gamma)$\n",
    "    - states $S$\n",
    "    - actions $A$ \n",
    "    - transition model $T(s_t, a_t, s_{t+1}) = p(s_{t+1} \\mid s_t, a_t)$\n",
    "    - reward function $R(s_t, a_t, s_{t+1}) \\in \\mathbb{R}$ \n",
    "    - reward discount factor $\\gamma \\in [0, 1]$ \n",
    "* Given a set of demonstrations $\\mathcal{D} = \\{\\tau_i\\}_{i=1}^{n}$ of an expert (agent), consisting of trajectories $\\tau = \\left( (s_1, a_1), (s_2, a_2), \\ldots, (s_{n-1}, a_{n-1}), s_n \\right)$ through the state-action space, IRL aims to recover the underlying reward function which explains the behavior of the expert.\n",
    "* $\\phi: S \\to \\mathbb{R}^d$ is a mapping from each state to a $d$-dimensional vector called features\n",
    "    - It can be extended to $\\phi: S \\times A \\to \\mathbb{R}^d$ (Scobee et al. 2020)\n",
    "    - For example in an MDP with 9 states, 4 actions, and 4 colors the feature vector is a concatenation of one-hot vectors of states, actions and colors for each $(s, a)$ \n",
    "* They also assume that the reward for each state(action) is defined as a linear combination of its features\n",
    "$$\n",
    "R(s) = \\omega^T\\phi(s)\n",
    "$$\n",
    "can be extended to \n",
    "$$\n",
    "R(s, a) = \\omega^T\\phi(s, a)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "R(\\tau) = \\sum_{s \\in \\tau} \\omega^T \\phi(s) = \\omega^T\\phi(\\tau)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem can be expressed as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathbb{E}_{\\pi^{Expert}}[\\phi(\\tau)] = \\mathbb{E}_{\\pi^{Learner}}[\\phi(\\tau)],\n",
    "$$\n",
    "or\n",
    "$$\n",
    "    \\mathbb{E}_{\\pi^{Expert}}[\\phi(\\tau)] = \\sum_{\\tau} p_{\\pi^{Learner}}(\\tau) \\phi(\\tau)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem can have many solutions, so they used Max Entropy to choose the solution with minimum bias. The result is\n",
    "\n",
    "$$\n",
    "    p(\\tau \\mid \\omega)\n",
    "    \\approx\n",
    "        \\frac{1}{Z(\\omega)} \\exp\\left( \\omega^\\top \\phi(\\tau) \\right)\n",
    "        \\prod_{s_{t+1}, a_t, s_t \\in \\tau} p(s_{t+1} \\mid s_t, a_t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then they use maximum likelihood to find the parameters ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing soft constraints to Max Entropy IRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an MDP of the nominal world and a set of demonstrations from the constrained world, we want to find the mapping $R^{r}: S \\times A \\to \\mathbb{R}_+$ which represents the amount of penalty for performing action $a$ in sates $s$ in the constrained world. This setting is expressive enough for state, action, and featrue constraints(Scobee et al. 2020). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can say\n",
    "\n",
    "$$\n",
    "R^{c}(s, a) = R^{n}(s, a) - R^{r}(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make the linear penalty assumtion, then we have\n",
    "\n",
    "$$\n",
    "R^{c}(s, a) = R^{n}(s, a) - \\omega^{r}\\phi(s, a)\n",
    "\\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently\n",
    "$$\n",
    "R^{c}(\\tau) = R^{n}(\\tau) - R^{r}(\\tau)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the Max Entropy IRL to find $\\omega^{r}$ by \n",
    "$$\n",
    "\\omega^{c} = \\omega^{n} - \\omega^{r}\n",
    "$$ \n",
    "where $\\omega^{nominal}$ can be found by running Max Entropy IRL on the nominal world. But it is not required as we can see later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient for Max Entropy IRL for the constrained world is\n",
    "\n",
    "$$\n",
    "\\nabla_c L(\\omega^{c}) = \\mathbb{E}_D[\\phi(\\tau)] - \\sum_{s_i} D^{c}_{s_i} \\phi(s_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathbb{E}_{D} \\left[ \\phi(\\tau) \\right]\n",
    "    = \\frac{1}{|\\mathcal{D}|} \\sum_{\\tau \\in \\mathcal{D}} \\sum_{s_t \\in \\tau} \\phi(s_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So,\n",
    "$$\n",
    "\\nabla_{r} L(\\omega^{c}) = \\nabla_r (\\omega_n - \\omega_r) \\times \\nabla_c L(\\omega^{c})  = -\\nabla_c L(\\omega^{c}) = \\sum_{s_i} D^{c}_{s_i} \\phi(s_i) -  \\mathbb{E}_D[\\phi(\\tau)]\n",
    "\\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\omega^{nominal}$ is a constant and not needed in the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: $MDP^{nominal}$, $D^{c}$\n",
    "\n",
    "Output: $\\omega^{rl}$\n",
    "1. Run the Max Entropy IRL\n",
    "    - use reward function in eq. (1) in the backward pass:\n",
    "    $$\n",
    "    Z_{a_{i,j}} = \\sum_k P(s_k|s_i, a_j)e^{R^{c}(s_i)} Z_{s_k}\n",
    "    $$\n",
    "        \n",
    "    - use gradient in eq. (2) in the optimization phase\n",
    "2. return the estimated $\\omega^{r}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ziebart, Brian D., et al. \"Maximum entropy inverse reinforcement learning.\" Aaai. Vol. 8. 2008.\n",
    "2. Scobee, Dexter RR, and S. Shankar Sastry. \"Maximum likelihood constraint inference for inverse reinforcement learning.\" arXiv preprint arXiv:1909.05477 (2019)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
